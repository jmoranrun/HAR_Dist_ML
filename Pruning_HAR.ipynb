{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Pruning_HAR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "environment": {
      "name": "tf2-2-2-gpu.2-2.m50",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmoranrun/HAR_Dist_ML/blob/main/Pruning_HAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Zt0JEl38SWX"
      },
      "source": [
        "#Pruning Experiments around Personalization for Human Actvity Recognition. \n",
        "I used the Karas Notebook \"Magnitude-based weight pruning with Keras\" as a reference.\n",
        "\n",
        "This notebook was used to run the experiments in the presentation:\n",
        "Human Activity Recognition in a Distributed Machine Learning System. Jul 26, 2021  IEEE International Conference on Wearable and Implantable Body Sensor Networks (BSN2021)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn836LSTNSHA"
      },
      "source": [
        "! pip uninstall -y tensorflow\n",
        "! pip uninstall -y tf-nightly\n",
        "#! pip install -U tensorflow-gpu==1.14.0\n",
        "! pip install -U tensorflow-gpu\n",
        "\n",
        "! pip install tensorflow-model-optimization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ykjgo4UNXmD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cca7ce93-4761-4c4c-cefe-4751d3bd4ddb"
      },
      "source": [
        "%load_ext tensorboard\n",
        "import tensorboard\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "import tensorflow as tf\n",
        "import random \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mydXeQlDNbnR"
      },
      "source": [
        "import tensorflow as tf\n",
        "#tf.enable_eager_execution()\n",
        "\n",
        "import tempfile\n",
        "import zipfile\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBYltugp-MdR"
      },
      "source": [
        "###Define functions\n",
        "Define functions to allow the dataset to be manipulated at a per-user level of ganularity. \n",
        "Also a function is defined to count the number of weights that are set to zero. The purpose of this function is to confirm the sparsity of the model. </br>\n",
        "This method was referenced from https://www.dlology.com/blog/how-to-compress-your-keras-model-x5-smaller-with-tensorflow-model-optimization/\n",
        "Note, the method to move data from the test to training dataset is not currently used in these experiments \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_MJqxz5z2dh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ee1a2e71-71e9-40aa-f1e3-8cd34baf238e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        " \n",
        "# load a single file as a numpy array\n",
        "def load_file(filepath):\n",
        "  dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
        "  return dataframe.values\n",
        " \n",
        "\n",
        "#######################################################################\n",
        "# Function to move selected test samples to traning dataset\n",
        "# Lots of parameters - however, these are driven by a higher level\n",
        "# function (select_user_val), which has a more user friendly parameter list\n",
        "####################################################################### \n",
        "def move_test_to_train(move_samples, move_sub_map, move_ys, move_idx, har_dataset_user_test, har_submap_user_test, har_dataset_user_train, har_submap_user_train, har_y_user_test, har_y_user_train):\n",
        "    har_dataset_user_test = np.asarray(har_dataset_user_test)\n",
        "    har_submap_user_test = np.asarray(har_submap_user_test)\n",
        "    har_dataset_user_train = np.asarray(har_dataset_user_train)\n",
        "    har_submap_user_train = np.asarray(har_submap_user_train)\n",
        "    har_y_user_test = np.asarray(har_y_user_test)\n",
        "    har_y_user_train = np.asarray(har_y_user_train)\n",
        "    move_samples = np.asarray(move_samples)\n",
        "    move_ys = np.asarray(move_ys)\n",
        "    har_dataset_user_test = np.delete(har_dataset_user_test, move_idx, axis=0)\n",
        "    har_submap_user_test = np.delete(har_submap_user_test, move_idx, axis=0)\n",
        "    har_y_user_test = np.delete(har_y_user_test, move_idx, axis=0)\n",
        "    har_dataset_user_train = np.concatenate((har_dataset_user_train, move_samples))\n",
        "    har_submap_user_train = np.concatenate((har_submap_user_train, move_sub_map))\n",
        "    har_y_user_train = np.concatenate((har_y_user_train, move_ys))\n",
        "    return  har_dataset_user_test, har_submap_user_test, har_y_user_test, har_dataset_user_train, har_submap_user_train, har_y_user_train\n",
        "\n",
        "#############################################################################################\n",
        "# Function to create a user dataset\n",
        "# har_dataset is a 3-D np array of form: [num_of_samples, num_of_timesamples, num_of_feature]\n",
        "#             Where num_of_samples is the total number of recorded activites \n",
        "#                   num_of_timesamples is the total number of timesammples in each recorded activity \n",
        "#                   num_of_feature is the total number of features recorded using the smartphones.\n",
        "# sub_map maps subjects (people) to each har_dataset sample\n",
        "# test_user lists which subjects are assigned to the test dataset\n",
        "# train_user lists which subjects are assigned to the training dataset\n",
        "# percent_mix determines the percentage of the test dataset samples to move into the training dataset\n",
        "#############################################################################################\n",
        "def select_user_val(har_dataset, y, sub_map, test_users, train_user, percent_mix):\n",
        "  har_dataset_user_test=[]\n",
        "  har_submap_user_test=[]\n",
        "  har_y_user_test=[]\n",
        "  for user in test_users:\n",
        "  #  har_dataset_user_test.append(har_dataset[tuple(np.where(sub_map==user))]) \n",
        "    har_dataset_user_test.extend(har_dataset[tuple(np.where(sub_map==user))].tolist()) \n",
        "    har_submap_user_test.extend(sub_map[np.where(sub_map==user)].tolist())\n",
        "    har_y_user_test.extend(y[tuple(np.where(sub_map==user))].tolist()) \n",
        "  # Generate the default training dataset \n",
        "  har_dataset_user_train=[]\n",
        "  har_submap_user_train=[]\n",
        "  har_y_user_train=[]\n",
        "  for user in train_user:\n",
        "    har_dataset_user_train.extend(har_dataset[tuple(np.where(sub_map==user))].tolist())\n",
        "    har_submap_user_train.extend(sub_map[np.where(sub_map==user)].tolist())\n",
        "    har_y_user_train.extend(y[tuple(np.where(sub_map==user))].tolist()) \n",
        " \n",
        "  ## Now allow a percentage of val users samples to enter the training dataset\n",
        "  ## Make sure that the percentage comes from each val user\n",
        " \n",
        "  for user in test_users:\n",
        "    har_user_sub_map=np.where(np.asarray(har_submap_user_test)==user)[0]\n",
        "    har_user_sub_map_cnt=np.count_nonzero(np.asarray(har_submap_user_test)==user)\n",
        "    num_take_sub_map = int(har_user_sub_map_cnt*percent_mix)\n",
        "    if(num_take_sub_map > 0) :\n",
        "      move_idx=random.sample(list(har_user_sub_map), num_take_sub_map)\n",
        "      move_samples=[har_dataset_user_test[i] for i in move_idx]  \n",
        "      move_sub_map=[har_submap_user_test[i] for i in move_idx]   \n",
        "      move_ys=[har_y_user_test[i] for i in move_idx]   \n",
        "      har_dataset_user_test, har_submap_user_test, har_y_user_test, har_dataset_user_train, har_submap_user_train, har_y_user_train \\\n",
        "        = move_test_to_train(move_samples, move_sub_map, move_ys, move_idx, har_dataset_user_test, har_submap_user_test, har_dataset_user_train, har_submap_user_train, har_y_user_test, har_y_user_train)\n",
        "  return  np.asarray(har_dataset_user_test), np.asarray(har_submap_user_test), np.asarray(har_y_user_test), np.asarray(har_dataset_user_train), np.asarray(har_submap_user_train), np.asarray(har_y_user_train)\n",
        " \n",
        " \n",
        "\n",
        "#https://www.dlology.com/blog/how-to-compress-your-keras-model-x5-smaller-with-tensorflow-model-optimization/ \n",
        "\n",
        "def check_zero_weights(model):\n",
        "  #print(model.get_weights())\n",
        "  for i, w in enumerate(model.get_weights()):\n",
        "    print(\n",
        "        \"{} -- Total:{}, Zeros: {:.2f}%\".format(\n",
        "            model.weights[i].name, w.size, np.sum(w == 0) / w.size * 100\n",
        "        )        \n",
        "    )\n",
        "    zero_idx=np.where(w == 0)\n",
        "    print(\"i iddex is zero_idx is\", i, zero_idx)  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1p_y3gPWeW2"
      },
      "source": [
        "\n",
        "### Define the CNN Model\n",
        "In this cell the CNN is defined. </br>\n",
        "It's initial weights are saved so that all experiments start from the same point to ensure fairness in the comparisions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLg0SGdYp2Q6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        },
        "outputId": "046cd218-6575-49da-d789-2a12a7d97276"
      },
      "source": [
        "##############################################\n",
        "#### Define the CNN ##########################        \n",
        "##############################################\n",
        "# set variables\n",
        "mix_users=True\n",
        "seg_len = 128\n",
        "num_channels = 9\n",
        "num_labels = 6\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "num_epoches = 0\n",
        "#num_batches = train_x.shape[0] // batch_size\n",
        "time_samples = 128\n",
        "feature_sensors = 9\n",
        "num_classes = 6\n",
        "\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "l = tf.keras.layers\n",
        "\n",
        "opt = 'sgd'\n",
        "l = tf.keras.layers\n",
        "model_har = tf.keras.Sequential([\n",
        "    l.Conv1D(filters=32,kernel_size=10,strides=1,activation='relu', input_shape=(time_samples, feature_sensors)),\n",
        "    l.MaxPooling1D(pool_size=4,strides=2, padding='same'),\n",
        "    l.Conv1D(filters=64,kernel_size=2,strides=1,activation='relu'),\n",
        "    l.MaxPooling1D(pool_size=4,strides=2, padding='same'),\n",
        "    l.Conv1D(filters=128,kernel_size=2,strides=2,activation='relu'),\n",
        "    l.Flatten(),\n",
        "    l.Dense(150),\n",
        "    l.Activation('tanh'),\n",
        "    l.Dropout(0.5),\n",
        "    l.Dense(50),\n",
        "    l.Activation('tanh'),\n",
        "    l.Dropout(0.5),\n",
        "    l.Dense(25),\n",
        "    l.Activation('tanh'),\n",
        "    l.Dropout(0.5),\n",
        "    l.Dense(num_labels),\n",
        "    l.Activation('softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model_har.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "## This NB needs to be run multiple times for one data collection due to \n",
        "## large computation effort.  Only write out the init weights once per\n",
        "## data collection run to keep comparisions equal\n",
        "gen_init_weights = False\n",
        "if(gen_init_weights):\n",
        "  model_har.save_weights('/content/drive/My Drive/model_har_pruning_init.h5')\n",
        "\n",
        "\n",
        "\n",
        "print(model_har.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_42 (Conv1D)           (None, 119, 32)           2912      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_28 (MaxPooling (None, 60, 32)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_43 (Conv1D)           (None, 59, 64)            4160      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_29 (MaxPooling (None, 30, 64)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_44 (Conv1D)           (None, 15, 128)           16512     \n",
            "_________________________________________________________________\n",
            "flatten_14 (Flatten)         (None, 1920)              0         \n",
            "_________________________________________________________________\n",
            "dense_56 (Dense)             (None, 150)               288150    \n",
            "_________________________________________________________________\n",
            "activation_56 (Activation)   (None, 150)               0         \n",
            "_________________________________________________________________\n",
            "dropout_42 (Dropout)         (None, 150)               0         \n",
            "_________________________________________________________________\n",
            "dense_57 (Dense)             (None, 50)                7550      \n",
            "_________________________________________________________________\n",
            "activation_57 (Activation)   (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dropout_43 (Dropout)         (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_58 (Dense)             (None, 25)                1275      \n",
            "_________________________________________________________________\n",
            "activation_58 (Activation)   (None, 25)                0         \n",
            "_________________________________________________________________\n",
            "dropout_44 (Dropout)         (None, 25)                0         \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 6)                 156       \n",
            "_________________________________________________________________\n",
            "activation_59 (Activation)   (None, 6)                 0         \n",
            "=================================================================\n",
            "Total params: 320,715\n",
            "Trainable params: 320,715\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg7eEb5Hb_N6"
      },
      "source": [
        "## Define The Pruning Flow\n",
        "This method is the backbone of the pruning experiments. </br>\n",
        "It creates edge and central datasets. </br>\n",
        "Pruning at the central node can be enabled/disabled through switch prune_at_center </br>\n",
        "Pruning at the edge nodes can be enabled/disabled through switch prune_at_edge </br>\n",
        "Training at the edge nodes can be enabled/disabled through switch train_at_edge </br>\n",
        "Training at the central nodes is always enabled </br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7c-fsQpTzOr"
      },
      "source": [
        "def run_prune(prune_at_edge, prune_at_center, train_at_edge, har_dataset, har_dataset_y, sub_map, edge_central_ratio, num_epoches = 25, eval_before_prune=False):\n",
        "\n",
        "  total_num_users=30\n",
        "  all_users = range(1, total_num_users)\n",
        "  edge_users = random.sample(all_users, int(edge_central_ratio*total_num_users))\n",
        "  #model_har.load_weights('/content/drive/My Drive/model_har_init.h5')\n",
        "  central_users=np.setdiff1d(all_users, edge_users)\n",
        "  print(\"Edge users are:\",edge_users)\n",
        "  print(\"Central users are:\",central_users)\n",
        "\n",
        "  percent_mix=0\n",
        "  edge_x, edge_submap, edge_y, central_x, central_submap, central_y = \\\n",
        "        select_user_val(har_dataset, har_dataset_y, sub_map, edge_users, central_users, percent_mix)\n",
        "\n",
        "  central_train_x, central_test_x, central_train_y, central_test_y = train_test_split(central_x, central_y, test_size = 0.2, random_state = 0,shuffle =True)\n",
        "  edge_train_x, edge_test_x, edge_train_y, edge_test_y = train_test_split(edge_x, edge_y, test_size = 0.2, random_state = 0,shuffle =True)\n",
        "\n",
        "  logdir = tempfile.mkdtemp()\n",
        "  print('Writing training logs to ' + logdir)\n",
        "  callbacks = [tf.keras.callbacks.TensorBoard(log_dir=logdir, profile_batch=0)]\n",
        "\n",
        "  \n",
        "  model_har.load_weights('/content/drive/My Drive/model_har_pruning_init.h5')\n",
        "  if(eval_before_prune== True):\n",
        "    model_har.fit(central_train_x, central_train_y,\n",
        "           batch_size=batch_size,\n",
        "           epochs=num_epoches,\n",
        "           verbose=0,\n",
        "           callbacks=callbacks,\n",
        "           validation_data=(central_test_x, central_test_y))\n",
        "    score = model_har.evaluate(central_test_x, central_test_y, verbose=0)\n",
        "    print('Test loss:', score[0])\n",
        "    print('Test accuracy:', score[1])\n",
        "    check_zero_weights(model_har)\n",
        "\n",
        "  num_train_samples = central_train_x.shape[0]\n",
        "  end_step = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * num_epoches\n",
        "  print('End step: ' + str(end_step))\n",
        "\n",
        "  ## jmoran hand set the pruning paras\n",
        "\n",
        "  if(prune_at_center==True):\n",
        "   pruning_params = {\n",
        "        'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.50,\n",
        "                                                   final_sparsity=0.75,\n",
        "                                                   begin_step=100,\n",
        "                                                   end_step=end_step,\n",
        "                                                   frequency=100)\n",
        "    }\n",
        "  else:\n",
        "    pruning_params = {\n",
        "        'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.0,\n",
        "                                                   final_sparsity=0.0,\n",
        "                                                   begin_step=100,\n",
        "                                                   end_step=end_step,\n",
        "                                                   frequency=100)\n",
        "   }\n",
        "\n",
        "\n",
        "  central_model_har = tf.keras.Sequential([\n",
        "    sparsity.prune_low_magnitude(l.Conv1D(filters=32,kernel_size=10,strides=1,activation='relu', input_shape=(time_samples, feature_sensors)), **pruning_params),\n",
        "    l.MaxPooling1D(pool_size=4,strides=2, padding='same'),\n",
        "    sparsity.prune_low_magnitude(l.Conv1D(filters=64,kernel_size=2,strides=1,activation='relu'),**pruning_params),\n",
        "    l.MaxPooling1D(pool_size=4,strides=2, padding='same'),\n",
        "    sparsity.prune_low_magnitude(l.Conv1D(filters=128,kernel_size=2,strides=2,activation='relu'),**pruning_params),\n",
        "    l.Flatten(),\n",
        "    sparsity.prune_low_magnitude(l.Dense(150),**pruning_params),\n",
        "    l.Activation('tanh'),\n",
        "    l.Dropout(0.5),\n",
        "    sparsity.prune_low_magnitude(l.Dense(50),**pruning_params),\n",
        "    l.Activation('tanh'),\n",
        "    l.Dropout(0.5),\n",
        "    sparsity.prune_low_magnitude(l.Dense(25),**pruning_params),\n",
        "    l.Activation('tanh'),\n",
        "    l.Dropout(0.5),\n",
        "    sparsity.prune_low_magnitude(l.Dense(num_labels),**pruning_params),\n",
        "    l.Activation('softmax')\n",
        "  ])\n",
        "\n",
        "  #pruned_model_har.summary()\n",
        "  central_model_har.compile(\n",
        "     loss=tf.keras.losses.categorical_crossentropy,\n",
        "     optimizer='adam',\n",
        "     metrics=['accuracy'])\n",
        "\n",
        "# Add a pruning step callback to peg the pruning step to the optimizer's\n",
        "# step. Also add a callback to add pruning summaries to tensorboard\n",
        "  callbacks = [\n",
        "     sparsity.UpdatePruningStep(),\n",
        "     sparsity.PruningSummaries(log_dir=logdir, profile_batch=0)\n",
        "  ]\n",
        "\n",
        "  central_model_har.fit(central_train_x, central_train_y,\n",
        "           batch_size=batch_size,\n",
        "           epochs=num_epoches,\n",
        "           verbose=0,\n",
        "           callbacks=callbacks,\n",
        "           validation_data=(central_test_x, central_test_y))\n",
        "\n",
        "  score = central_model_har.evaluate(central_test_x, central_test_y, verbose=0)\n",
        "  print('Test loss:', score[0])\n",
        "  print('Test accuracy:', score[1])\n",
        "  check_zero_weights(central_model_har)\n",
        "  central_model_har.save_weights('/content/drive/My Drive/pruned_model_har_central.h5')\n",
        "\n",
        "  if(prune_at_edge==True):\n",
        "   pruning_params_edge = {\n",
        "         'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.50,\n",
        "                                                    final_sparsity=0.75,\n",
        "                                                     begin_step=100,\n",
        "                                                    end_step=end_step,\n",
        "                                                     frequency=100)\n",
        "   }\n",
        "  else:\n",
        "   pruning_params_edge = {\n",
        "         'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.0,\n",
        "                                                    final_sparsity=0.0,\n",
        "                                                    begin_step=100,\n",
        "                                                    end_step=end_step,\n",
        "                                                    frequency=100)\n",
        "   }\n",
        "\n",
        "  edge_pruned_model_har = tf.keras.Sequential([\n",
        "     sparsity.prune_low_magnitude(l.Conv1D(filters=32,kernel_size=10,strides=1,activation='relu', input_shape=(time_samples, feature_sensors)), **pruning_params_edge),\n",
        "     l.MaxPooling1D(pool_size=4,strides=2, padding='same'),\n",
        "     sparsity.prune_low_magnitude(l.Conv1D(filters=64,kernel_size=2,strides=1,activation='relu'),**pruning_params_edge),\n",
        "     l.MaxPooling1D(pool_size=4,strides=2, padding='same'),\n",
        "     sparsity.prune_low_magnitude(l.Conv1D(filters=128,kernel_size=2,strides=2,activation='relu'),**pruning_params_edge),\n",
        "     l.Flatten(),\n",
        "      sparsity.prune_low_magnitude(l.Dense(150),**pruning_params_edge),\n",
        "     l.Activation('tanh'),\n",
        "     l.Dropout(0.5),\n",
        "     sparsity.prune_low_magnitude(l.Dense(50),**pruning_params_edge),\n",
        "     l.Activation('tanh'),\n",
        "     l.Dropout(0.5),\n",
        "     sparsity.prune_low_magnitude(l.Dense(25),**pruning_params_edge),\n",
        "     l.Activation('tanh'),\n",
        "     l.Dropout(0.5),\n",
        "     sparsity.prune_low_magnitude(l.Dense(num_labels),**pruning_params_edge),\n",
        "      l.Activation('softmax')\n",
        "  ])\n",
        "\n",
        "  edge_pruned_model_har.compile(\n",
        "    loss=tf.keras.losses.categorical_crossentropy,\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "  if(prune_at_edge==True):          ## If Prune at edge take the model which has not had pruning done and apply the appriate weights from the central model (pruning can't be appied twice to a model)\n",
        "   edge_pruned_model_har=edge_pruned_model_har\n",
        "   edge_pruned_model_har.load_weights('/content/drive/My Drive/pruned_model_har_central.h5')\n",
        "  else:\n",
        "   edge_pruned_model_har=central_model_har\n",
        "\n",
        "# Add a pruning step callback to peg the pruning step to the optimizer's\n",
        "# step. Also add a callback to add pruning summaries to tensorboard\n",
        "  callbacks = [\n",
        "     sparsity.UpdatePruningStep(),\n",
        "     sparsity.PruningSummaries(log_dir=logdir, profile_batch=0)\n",
        "  ]\n",
        "\n",
        "  if(train_at_edge==True):\n",
        "   edge_pruned_model_har.fit(edge_train_x, edge_train_y,\n",
        "           batch_size=batch_size,\n",
        "           epochs=num_epoches,\n",
        "           verbose=0,\n",
        "           callbacks=callbacks,\n",
        "           validation_data=(edge_test_x, edge_test_y))\n",
        "\n",
        "  score = edge_pruned_model_har.evaluate(edge_test_x, edge_test_y, verbose=0)\n",
        "  print('Edge Test loss:', score[0])\n",
        "  print('Edge Test accuracy:', score[1])\n",
        "  edge_acc =  score[1]\n",
        "\n",
        "  score = central_model_har.evaluate(central_test_x, central_test_y, verbose=0)\n",
        "  print('Central Test loss:', score[0])\n",
        "  print('Central accuracy:', score[1])\n",
        "  central_acc =  score[1]\n",
        "  print(edge_pruned_model_har.summary())\n",
        "  check_zero_weights(edge_pruned_model_har)\n",
        "\n",
        "  return  central_acc, edge_acc \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YvTSXw3gLqW"
      },
      "source": [
        "### Prepare The Dataset and set the Run_prune parameters\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgzYvhRAc6Y4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "3c2118f9-0152-4e0e-c5ff-9fe869f986e4"
      },
      "source": [
        "\n",
        "## This files contains the training users mapping\n",
        "sub_map_train = load_file('/content/drive/My Drive/Colab Notebooks/UCI_dataset/UCI HAR Dataset/train/subject_train.txt')\n",
        "train_subjects = np.unique(sub_map_train)\n",
        "print(train_subjects)\n",
        " \n",
        "sub_map_test = load_file('/content/drive/My Drive/Colab Notebooks/UCI_dataset/UCI HAR Dataset/test/subject_test.txt')\n",
        "test_subjects = np.unique(sub_map_test)\n",
        "print(test_subjects)\n",
        " \n",
        " \n",
        "har_features = [\n",
        "    \"body_acc_x\",\n",
        "    \"body_acc_y\",\n",
        "    \"body_acc_z\",\n",
        "    \"body_gyro_x\",\n",
        "    \"body_gyro_y\",\n",
        "    \"body_gyro_z\",\n",
        "    \"total_acc_x\",\n",
        "    \"total_acc_y\",\n",
        "    \"total_acc_z\"\n",
        "]\n",
        " \n",
        "num_of_timesamples = 128\n",
        "num_of_feature = len(har_features)\n",
        "num_of_samples = sub_map_train.shape[0] + sub_map_test.shape[0]\n",
        "print(\"num_of_samples is\",num_of_samples)\n",
        "\n",
        "sub_map = np.concatenate((sub_map_test, sub_map_train), axis =0)\n",
        "sub_map = sub_map.reshape(-1) \n",
        "print(sub_map)\n",
        "unique, counts = np.unique(sub_map, return_counts=True)\n",
        "print(dict(zip(unique, counts)))\n",
        " \n",
        "har_feature_cnt=0\n",
        "har_dataset =  np.empty([num_of_samples, num_of_timesamples, num_of_feature])\n",
        "for har_feature in har_features:\n",
        "   har_feature_test  = load_file(f'/content/drive/My Drive/Colab Notebooks/UCI_dataset/UCI HAR Dataset/test/Inertial Signals/{har_feature}_test.txt')\n",
        "   har_feature_train = load_file(f'/content/drive/My Drive/Colab Notebooks/UCI_dataset/UCI HAR Dataset/train/Inertial Signals/{har_feature}_train.txt')\n",
        "   har_feature = np.concatenate((har_feature_test, har_feature_train), axis=0)\n",
        "   har_dataset[:, :, har_feature_cnt] = har_feature \n",
        "   har_feature_cnt += 1\n",
        " \n",
        "har_dataset_with_subject = np.expand_dims(har_dataset, axis=1)\n",
        "print(\"har_dataset_with_subject.shape is\",har_dataset_with_subject.shape)\n",
        " \n",
        "train_y =  load_file('/content/drive/My Drive/Colab Notebooks/UCI_dataset/UCI HAR Dataset/train/y_train.txt')\n",
        "test_y =  load_file('/content/drive/My Drive/Colab Notebooks/UCI_dataset/UCI HAR Dataset/test/y_test.txt')\n",
        "train_y = np.asarray(pd.get_dummies(train_y.reshape(-1, )))\n",
        "test_y = np.asarray(pd.get_dummies(test_y.reshape(-1, )))\n",
        "har_dataset_y = np.concatenate((test_y,train_y),axis=0)\n",
        "\n",
        "num_runs_per_edge_ratio=20\n",
        "edge_central_ratio_lst = [0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1  3  5  6  7  8 11 14 15 16 17 19 21 22 23 25 26 27 28 29 30]\n",
            "[ 2  4  9 10 12 13 18 20 24]\n",
            "num_of_samples is 10299\n",
            "[ 2  2  2 ... 30 30 30]\n",
            "{1: 347, 2: 302, 3: 341, 4: 317, 5: 302, 6: 325, 7: 308, 8: 281, 9: 288, 10: 294, 11: 316, 12: 320, 13: 327, 14: 323, 15: 328, 16: 366, 17: 368, 18: 364, 19: 360, 20: 354, 21: 408, 22: 321, 23: 372, 24: 381, 25: 409, 26: 392, 27: 376, 28: 382, 29: 344, 30: 383}\n",
            "har_dataset_with_subject.shape is (10299, 1, 128, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh8DNMHKg5wH"
      },
      "source": [
        "### Run Pruning experiment with: <br>\n",
        "prune_at_edge=False<br>\n",
        "prune_at_center=False<br>\n",
        "train_at_edge=False<br> \n",
        "Results will be printed to file \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4JAWtcDRKtY"
      },
      "source": [
        "prune_at_edge=False\n",
        "prune_at_center=False\n",
        "train_at_edge=False\n",
        "\n",
        "\n",
        "central_acc_lst=[]\n",
        "edge_acc_lst=[]\n",
        "\n",
        "for edge_central_ratio in edge_central_ratio_lst:\n",
        "  for run in range(num_runs_per_edge_ratio):\n",
        "    central_acc, edge_acc = run_prune(prune_at_edge=prune_at_edge, prune_at_center=prune_at_center, train_at_edge=train_at_edge, har_dataset=har_dataset, har_dataset_y=har_dataset_y, sub_map=sub_map, edge_central_ratio=edge_central_ratio)\n",
        "    central_acc_lst.append(central_acc)\n",
        "    edge_acc_lst.append(edge_acc)\n",
        "\n",
        "with open('/content/drive/My Drive/pef_pcf_tef_bl_20.txt', 'w') as f:\n",
        "        f.writelines(\"%s\\n\" % central_acc_lst)\n",
        "        f.writelines(\"%s\\n\" % edge_acc_lst)   \n",
        "print(\"central_acc_lst is\", central_acc_lst)\n",
        "print(\"edge_acc_lst is\",edge_acc_lst)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk8KiMYjhemh"
      },
      "source": [
        "### Run Pruning experiment with: <br>\n",
        "prune_at_edge=False<br>\n",
        "prune_at_center=True<br>\n",
        "train_at_edge=False<br> \n",
        "Results will be printed to file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNTnihIBvv3h"
      },
      "source": [
        "\n",
        "prune_at_edge=False\n",
        "prune_at_center=True\n",
        "train_at_edge=False\n",
        "\n",
        "central_acc_lst1=[]\n",
        "edge_acc_lst1=[]\n",
        "\n",
        "\n",
        "for edge_central_ratio in edge_central_ratio_lst:\n",
        "  for run in range(num_runs_per_edge_ratio):\n",
        "    central_acc, edge_acc = run_prune(prune_at_edge=prune_at_edge, prune_at_center=prune_at_center, train_at_edge=train_at_edge, har_dataset=har_dataset, har_dataset_y=har_dataset_y, sub_map=sub_map, edge_central_ratio=edge_central_ratio)\n",
        "    central_acc_lst1.append(central_acc)\n",
        "    edge_acc_lst1.append(edge_acc)\n",
        "\n",
        "with open('/content/drive/My Drive/pef_pct_tef_bl_20.txt', 'w') as f:\n",
        "        f.writelines(\"%s\\n\" % central_acc_lst1)\n",
        "        f.writelines(\"%s\\n\" % edge_acc_lst1)   \n",
        "\n",
        "print(\"central_acc_lst is\", central_acc_lst1)\n",
        "print(\"edge_acc_lst is\",edge_acc_lst1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O-opJ8Ihiqn"
      },
      "source": [
        "### Run Pruning experiment with: <br>\n",
        "prune_at_edge=False<br>\n",
        "prune_at_center=True<br>\n",
        "train_at_edge=True<br> \n",
        "Results will be printed to file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdyF5a8YRzdt"
      },
      "source": [
        "\n",
        "prune_at_edge=False\n",
        "prune_at_center=True\n",
        "train_at_edge=True\n",
        "\n",
        "central_acc_lst2=[]\n",
        "edge_acc_lst2=[]\n",
        "\n",
        "for edge_central_ratio in edge_central_ratio_lst:\n",
        "  for run in range(num_runs_per_edge_ratio):\n",
        "    central_acc, edge_acc = run_prune(prune_at_edge=prune_at_edge, prune_at_center=prune_at_center, train_at_edge=train_at_edge, har_dataset=har_dataset, har_dataset_y=har_dataset_y, sub_map=sub_map, edge_central_ratio=edge_central_ratio)\n",
        "    central_acc_lst2.append(central_acc)\n",
        "    edge_acc_lst2.append(edge_acc)\n",
        "\n",
        "with open('/content/drive/My Drive/pef_pct_tet_bl_20.txt', 'w') as f:\n",
        "        f.writelines(\"%s\\n\" % central_acc_lst2)\n",
        "        f.writelines(\"%s\\n\" % edge_acc_lst2)   \n",
        "\n",
        "print(\"central_acc_lst is\", central_acc_lst2)\n",
        "print(\"edge_acc_lst is\",edge_acc_lst2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4KhNqquhoiI"
      },
      "source": [
        "### Run Pruning experiment with: <br>\n",
        "prune_at_edge=True<br>\n",
        "prune_at_center=False<br>\n",
        "train_at_edge=True<br> \n",
        "Results will be printed to file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ee4U90NXSCKD"
      },
      "source": [
        "\n",
        "prune_at_edge=True\n",
        "prune_at_center=False\n",
        "train_at_edge=True\n",
        "\n",
        "central_acc_lst3=[]\n",
        "edge_acc_lst3=[]\n",
        "\n",
        "for edge_central_ratio in edge_central_ratio_lst:\n",
        "  for run in range(num_runs_per_edge_ratio):\n",
        "    central_acc, edge_acc = run_prune(prune_at_edge=prune_at_edge, prune_at_center=prune_at_center, train_at_edge=train_at_edge, har_dataset=har_dataset, har_dataset_y=har_dataset_y, sub_map=sub_map, edge_central_ratio=edge_central_ratio)\n",
        "    central_acc_lst3.append(central_acc)\n",
        "    edge_acc_lst3.append(edge_acc)\n",
        "\n",
        "with open('/content/drive/My Drive/pet_pcf_tet_bl_20.txt', 'w') as f:\n",
        "        f.writelines(\"%s\\n\" % central_acc_lst3)\n",
        "        f.writelines(\"%s\\n\" % edge_acc_lst3)   \n",
        "\n",
        "print(\"central_acc_lst is\", central_acc_lst3)\n",
        "print(\"edge_acc_lst is\",edge_acc_lst3)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}